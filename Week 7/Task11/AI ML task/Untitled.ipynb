{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ec3206a-eb74-4bad-bb7b-fc61f953955a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# 1. Load the dataset using Pandas\n",
    "file_path = \"C:\\\\Users\\\\Dell\\\\Desktop\\\\New folder\\\\SMSSpamCollection\"\n",
    "df = pd.read_csv(file_path, sep='\\t', header=None, names=['label', 'message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47ccbe6d-0266-4007-ac2c-7a9e6d5720ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   label    5572 non-null   object\n",
      " 1   message  5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b19abc2-ca4f-4468-a870-c9e8691ea5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      "label      0\n",
      "message    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 4. Identify and handle any missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values:\")\n",
    "print(missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39278428-e881-4da9-a842-6094a5f163d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 403\n",
      "Dataset after removing duplicates: (5169, 2)\n"
     ]
    }
   ],
   "source": [
    "# 5. Check for duplicate rows and remove them if necessary\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n",
    "df_cleaned = df.drop_duplicates()\n",
    "print(f\"Dataset after removing duplicates: {df_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22c25117-5af5-4fbb-a4fc-eb8a451d59a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution:\n",
      "label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 6. Calculate the distribution of class labels (e.g., 'ham' vs. 'spam')\n",
    "label_distribution = df['label'].value_counts()\n",
    "print(\"Label distribution:\")\n",
    "print(label_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e4371ed-e068-4a84-be7f-afd1115f7a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imbalance analysis:\n",
      "label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 7. Analyze dataset imbalance and perform data balancing\n",
    "imbalance_analysis = label_distribution\n",
    "print(\"Imbalance analysis:\")\n",
    "print(imbalance_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b02c2548-eaca-4e98-82c5-397a53c37b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequently occurring words in the entire dataset:\n",
      "['free' 'good' 'gt' 'just' 'know' 'like' 'll' 'lt' 'ok' 'ur']\n"
     ]
    }
   ],
   "source": [
    "# 8. Identify the most frequently occurring words in the dataset\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=10)\n",
    "X = vectorizer.fit_transform(df['message'])\n",
    "frequent_words = vectorizer.get_feature_names_out()\n",
    "print(\"Most frequently occurring words in the entire dataset:\")\n",
    "print(frequent_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8200fb58-376d-41f5-868c-5a8af651682d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in 'spam' messages:\n",
      "['claim' 'free' 'mobile' 'prize' 'reply' 'stop' 'text' 'txt' 'ur' 'www']\n"
     ]
    }
   ],
   "source": [
    "# 9. Identify the most common words in 'spam' messages\n",
    "spam_messages = df[df['label'] == 'spam']['message']\n",
    "spam_vectorizer = CountVectorizer(stop_words='english', max_features=10)\n",
    "spam_X = spam_vectorizer.fit_transform(spam_messages)\n",
    "spam_frequent_words = spam_vectorizer.get_feature_names_out()\n",
    "print(\"Most common words in 'spam' messages:\")\n",
    "print(spam_frequent_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f3b6f07-05e8-4d78-8847-e4b9637aeced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in 'ham' messages:\n",
      "['good' 'got' 'gt' 'just' 'know' 'like' 'll' 'lt' 'ok' 'ur']\n"
     ]
    }
   ],
   "source": [
    "# 10. Identify the most common words in 'ham' messages\n",
    "ham_messages = df[df['label'] == 'ham']['message']\n",
    "ham_vectorizer = CountVectorizer(stop_words='english', max_features=10)\n",
    "ham_X = ham_vectorizer.fit_transform(ham_messages)\n",
    "ham_frequent_words = ham_vectorizer.get_feature_names_out()\n",
    "print(\"Most common words in 'ham' messages:\")\n",
    "print(ham_frequent_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6743ff62-97c4-42c6-8afa-87a25922943b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d64c4c0-e4b4-43c8-b188-1084b8c040d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the dataset\n",
    "file_path = \"C:\\\\Users\\\\Dell\\\\Desktop\\\\New folder\\\\SMSSpamCollection\"\n",
    "df = pd.read_csv(file_path, sep='\\t', header=None, names=['label', 'message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "455ff0bd-489a-4773-a708-dc64f034b4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['message'] = df['message'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d88c675e-d734-4add-9bad-97804313296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Remove Punctuation, Special Characters, and Numbers\n",
    "df['message'] = df['message'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c197da1-bc94-4f5a-8c6d-16f176f88db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Tokenize Messages into Words (using NLTK word_tokenize, or simply split by spaces)\n",
    "df['message'] = df['message'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9de79829-aa27-4f09-a7f0-c45d48f1d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Remove Stop Words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['message'] = df['message'].apply(lambda x: [word for word in x if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ddd77bd-8f52-44ac-90dd-2333109ddf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "df['message_stemmed'] = df['message'].apply(lambda x: [stemmer.stem(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1b1e23ae-ffd5-4549-8c7b-754e14d57f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "df['message_lemmatized'] = df['message'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7bb6135e-5d7a-4675-b748-114d2aa6a583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Convert Text to Numerical Form using TF-IDF\n",
    "# Join words back together to pass into TF-IDF\n",
    "df['message_joined'] = df['message_lemmatized'].apply(lambda x: ' '.join(x))\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = tfidf_vectorizer.fit_transform(df['message_joined'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d56c9c0d-9369-4b8d-a705-da82e4e9a1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                            message  \\\n",
      "0   ham  [go, jurong, point, crazy, available, bugis, n...   \n",
      "1   ham                     [ok, lar, joking, wif, u, oni]   \n",
      "2  spam  [free, entry, wkly, comp, win, fa, cup, final,...   \n",
      "3   ham      [u, dun, say, early, hor, u, c, already, say]   \n",
      "4   ham  [nah, dont, think, goes, usf, lives, around, t...   \n",
      "\n",
      "                                     message_stemmed  \\\n",
      "0  [go, jurong, point, crazi, avail, bugi, n, gre...   \n",
      "1                       [ok, lar, joke, wif, u, oni]   \n",
      "2  [free, entri, wkli, comp, win, fa, cup, final,...   \n",
      "3      [u, dun, say, earli, hor, u, c, alreadi, say]   \n",
      "4  [nah, dont, think, goe, usf, live, around, tho...   \n",
      "\n",
      "                                  message_lemmatized  \\\n",
      "0  [go, jurong, point, crazy, available, bugis, n...   \n",
      "1                     [ok, lar, joking, wif, u, oni]   \n",
      "2  [free, entry, wkly, comp, win, fa, cup, final,...   \n",
      "3      [u, dun, say, early, hor, u, c, already, say]   \n",
      "4  [nah, dont, think, go, usf, life, around, though]   \n",
      "\n",
      "                                      message_joined  \n",
      "0  go jurong point crazy available bugis n great ...  \n",
      "1                            ok lar joking wif u oni  \n",
      "2  free entry wkly comp win fa cup final tkts st ...  \n",
      "3                u dun say early hor u c already say  \n",
      "4           nah dont think go usf life around though  \n",
      "TF-IDF Features:\n",
      "['abiola' 'able' 'abt' 'accept' 'access' 'account' 'across' 'actually'\n",
      " 'add' 'address' 'admirer' 'aft' 'afternoon' 'age' 'ago' 'ah' 'aight'\n",
      " 'aint' 'almost' 'alone' 'already' 'alright' 'also' 'always' 'amp' 'ampm'\n",
      " 'an' 'angry' 'another' 'answer' 'anymore' 'anyone' 'anything' 'anytime'\n",
      " 'anyway' 'apartment' 'apply' 'ard' 'area' 'around' 'as' 'asap' 'ask'\n",
      " 'askd' 'asked' 'asking' 'attempt' 'auction' 'available' 'ave' 'await'\n",
      " 'awaiting' 'award' 'awarded' 'away' 'awesome' 'babe' 'baby' 'back' 'bad'\n",
      " 'bag' 'bak' 'bank' 'bath' 'bathe' 'bb' 'bcoz' 'bday' 'beautiful' 'bed'\n",
      " 'believe' 'best' 'better' 'bid' 'big' 'bill' 'birthday' 'bit' 'blue'\n",
      " 'bluetooth' 'bold' 'bonus' 'book' 'booked' 'bored' 'bos' 'bout' 'box'\n",
      " 'boy' 'boytoy' 'break' 'bring' 'brings' 'brother' 'bslvyl' 'bt' 'bus'\n",
      " 'busy' 'buy' 'buying' 'call' 'called' 'caller' 'callertune' 'calling'\n",
      " 'camcorder' 'came' 'camera' 'cant' 'car' 'card' 'care' 'carlos' 'case'\n",
      " 'cash' 'catch' 'cause' 'cd' 'chance' 'change' 'charge' 'charged'\n",
      " 'charity' 'chat' 'cheap' 'check' 'checking' 'cheer' 'chennai' 'chikku'\n",
      " 'child' 'choose' 'christmas' 'claim' 'class' 'close' 'club' 'co' 'code'\n",
      " 'coffee' 'colleague' 'collect' 'collection' 'college' 'colour' 'come'\n",
      " 'comin' 'coming' 'comp' 'company' 'completely' 'complimentary' 'computer'\n",
      " 'confirm' 'congrats' 'congratulation' 'contact' 'content' 'convey' 'cool'\n",
      " 'copy' 'correct' 'cost' 'could' 'couple' 'course' 'cover' 'coz' 'crave'\n",
      " 'crazy' 'credit' 'cum' 'cup' 'currently' 'custcare' 'customer' 'cut'\n",
      " 'cute' 'da' 'dad' 'daddy' 'darlin' 'darren' 'dat' 'date' 'dating' 'day'\n",
      " 'de' 'deal' 'dear' 'decide' 'decided' 'deep' 'del' 'delivery' 'den'\n",
      " 'detail' 'didnt' 'die' 'different' 'difficult' 'digital' 'dinner'\n",
      " 'direct' 'dis' 'discount' 'disturb' 'dnt' 'doctor' 'doesnt' 'dog'\n",
      " 'dogging' 'doin' 'done' 'dont' 'door' 'double' 'download' 'draw' 'dream'\n",
      " 'drink' 'drive' 'driving' 'drop' 'drug' 'dude' 'dun' 'dunno' 'dvd'\n",
      " 'earlier' 'early' 'easy' 'eat' 'eg' 'eh' 'either' 'else' 'em' 'email'\n",
      " 'empty' 'end' 'ending' 'energy' 'england' 'enjoy' 'enough' 'enter'\n",
      " 'entered' 'entry' 'especially' 'etc' 'eve' 'even' 'evening' 'ever'\n",
      " 'every' 'everyone' 'everything' 'ex' 'exam' 'excuse' 'expires' 'extra'\n",
      " 'eye' 'face' 'facebook' 'fact' 'fall' 'family' 'fancy' 'fantastic'\n",
      " 'fantasy' 'far' 'fast' 'father' 'feel' 'feeling' 'felt' 'fight' 'figure'\n",
      " 'file' 'fill' 'film' 'final' 'finally' 'find' 'fine' 'finish' 'finished'\n",
      " 'first' 'fixed' 'flag' 'flight' 'fone' 'food' 'forever' 'forget' 'forgot'\n",
      " 'forwarded' 'found' 'fr' 'free' 'freemsg' 'frens' 'fri' 'friday' 'friend'\n",
      " 'friendship' 'frm' 'frnd' 'frnds' 'fuck' 'fucking' 'full' 'fun' 'gal'\n",
      " 'game' 'gap' 'gas' 'gave' 'gay' 'gd' 'get' 'gettin' 'getting' 'gift'\n",
      " 'girl' 'give' 'glad' 'go' 'god' 'goin' 'going' 'gone' 'gonna' 'good'\n",
      " 'goodmorning' 'goodnight' 'got' 'goto' 'gotta' 'gr' 'great' 'grin'\n",
      " 'guaranteed' 'gud' 'guess' 'guy' 'gym' 'ha' 'haf' 'haha' 'hair' 'half'\n",
      " 'hand' 'happen' 'happened' 'happiness' 'happy' 'hard' 'hav' 'havent' 'he'\n",
      " 'head' 'hear' 'heard' 'heart' 'hee' 'hell' 'hello' 'help' 'hey' 'hi'\n",
      " 'hit' 'hiya' 'hl' 'hmm' 'hmmm' 'hmv' 'ho' 'hold' 'holiday' 'home' 'hope'\n",
      " 'hoping' 'hospital' 'hot' 'hotel' 'hour' 'house' 'however' 'hows' 'hr'\n",
      " 'huh' 'hungry' 'hurt' 'ice' 'id' 'idea' 'identifier' 'ill' 'im'\n",
      " 'immediately' 'important' 'india' 'indian' 'info' 'information' 'invited'\n",
      " 'ipod' 'ish' 'isnt' 'ive' 'jay' 'job' 'john' 'join' 'joke' 'joy' 'jus'\n",
      " 'juz' 'kate' 'keep' 'kick' 'kid' 'kind' 'kinda' 'king' 'kiss' 'knew'\n",
      " 'know' 'knw' 'la' 'lady' 'land' 'landline' 'laptop' 'lar' 'last' 'late'\n",
      " 'later' 'latest' 'laugh' 'lazy' 'ldn' 'le' 'leaf' 'least' 'leave'\n",
      " 'leaving' 'lect' 'left' 'leh' 'lei' 'lemme' 'lesson' 'let' 'letter'\n",
      " 'liao' 'library' 'life' 'light' 'like' 'line' 'link' 'list' 'listen'\n",
      " 'little' 'live' 'load' 'loan' 'log' 'lol' 'long' 'look' 'looking' 'lor'\n",
      " 'lose' 'lost' 'lot' 'lovable' 'love' 'loved' 'lovely' 'lover' 'loving'\n",
      " 'ltd' 'ltdecimalgt' 'ltgt' 'luck' 'lucky' 'lunch' 'luv' 'made' 'mah'\n",
      " 'mail' 'make' 'making' 'man' 'many' 'march' 'mark' 'match' 'mate' 'may'\n",
      " 'mayb' 'maybe' 'mean' 'meant' 'med' 'meet' 'meeting' 'meh' 'member' 'men'\n",
      " 'merry' 'message' 'met' 'might' 'min' 'mind' 'mine' 'minute' 'miss'\n",
      " 'missed' 'missing' 'mistake' 'mm' 'mob' 'mobile' 'mobileupd' 'mode' 'mom'\n",
      " 'moment' 'mon' 'monday' 'money' 'month' 'mood' 'moon' 'morning'\n",
      " 'motorola' 'move' 'movie' 'mp' 'mr' 'mrng' 'mrt' 'msg' 'mths' 'mu' 'much'\n",
      " 'mum' 'music' 'must' 'muz' 'na' 'nah' 'name' 'national' 'nd' 'near'\n",
      " 'need' 'network' 'neva' 'never' 'new' 'news' 'next' 'ni' 'nice' 'nigeria'\n",
      " 'night' 'nite' 'nobody' 'noe' 'nokia' 'noon' 'nope' 'normal' 'nothing'\n",
      " 'nt' 'ntt' 'number' 'nyt' 'offer' 'office' 'oh' 'ok' 'okay' 'okie' 'old'\n",
      " 'one' 'online' 'oops' 'open' 'operator' 'opinion' 'opt' 'optout' 'orange'\n",
      " 'orchard' 'order' 'oredi' 'oso' 'others' 'otherwise' 'outside' 'pa'\n",
      " 'page' 'pain' 'paper' 'parent' 'park' 'part' 'party' 'pas' 'pay' 'people'\n",
      " 'per' 'person' 'pete' 'phone' 'photo' 'pic' 'pick' 'picking' 'pizza'\n",
      " 'place' 'plan' 'planning' 'play' 'player' 'please' 'pls' 'plus' 'plz'\n",
      " 'pm' 'pmin' 'pmsg' 'po' 'pobox' 'point' 'police' 'poly' 'polys' 'poor'\n",
      " 'possible' 'post' 'pound' 'ppm' 'press' 'pretty' 'price' 'princess'\n",
      " 'private' 'prize' 'prob' 'probably' 'problem' 'project' 'promise' 'pub'\n",
      " 'put' 'question' 'quick' 'quite' 'quiz' 'rain' 'rate' 'rd' 'reach'\n",
      " 'reached' 'reaching' 'read' 'reading' 'ready' 'real' 'really' 'reason'\n",
      " 'receive' 'record' 'reference' 'registered' 'remember' 'remove' 'rent'\n",
      " 'rental' 'reply' 'replying' 'representative' 'request' 'rest' 'result'\n",
      " 'return' 'reward' 'right' 'ring' 'ringtone' 'rite' 'road' 'rock' 'room'\n",
      " 'rose' 'round' 'rply' 'rreveal' 'run' 'sad' 'sae' 'safe' 'said' 'sale'\n",
      " 'sat' 'saturday' 'savamob' 'save' 'saw' 'say' 'saying' 'sch' 'school'\n",
      " 'sea' 'search' 'second' 'secret' 'see' 'seeing' 'seems' 'seen' 'selected'\n",
      " 'self' 'sell' 'semester' 'send' 'sending' 'sent' 'seriously' 'service'\n",
      " 'set' 'sex' 'sexy' 'shall' 'shes' 'shit' 'shop' 'shopping' 'short' 'show'\n",
      " 'shower' 'si' 'sick' 'side' 'silent' 'sim' 'simple' 'since' 'single'\n",
      " 'sir' 'sister' 'sitting' 'situation' 'sleep' 'sleeping' 'slow' 'slowly'\n",
      " 'sm' 'small' 'smile' 'smiling' 'smoke' 'smth' 'snow' 'sofa' 'somebody'\n",
      " 'someone' 'something' 'somewhere' 'song' 'sony' 'soon' 'sorry' 'sort'\n",
      " 'sound' 'speak' 'special' 'specialcall' 'specially' 'spend' 'spent'\n",
      " 'sport' 'st' 'stand' 'start' 'started' 'starting' 'statement' 'stay'\n",
      " 'staying' 'std' 'still' 'stop' 'store' 'story' 'street' 'study'\n",
      " 'studying' 'stuff' 'stupid' 'style' 'sub' 'summer' 'sun' 'sunshine'\n",
      " 'support' 'supposed' 'sure' 'surprise' 'sweet' 'swing' 'take' 'taking'\n",
      " 'talk' 'talking' 'tampa' 'tc' 'teach' 'tear' 'tel' 'tell' 'telling' 'ten'\n",
      " 'term' 'test' 'text' 'th' 'thank' 'thanks' 'thanx' 'thats' 'there'\n",
      " 'theyre' 'thing' 'think' 'thinkin' 'thinking' 'thk' 'tho' 'though'\n",
      " 'thought' 'tht' 'ticket' 'til' 'till' 'time' 'tired' 'tmr' 'today'\n",
      " 'together' 'told' 'tomo' 'tomorrow' 'tone' 'tonight' 'took' 'top' 'tot'\n",
      " 'touch' 'town' 'train' 'treat' 'tried' 'trip' 'true' 'trust' 'truth'\n",
      " 'try' 'trying' 'tscs' 'tuesday' 'turn' 'tv' 'two' 'txt' 'txting' 'txts'\n",
      " 'type' 'ufind' 'ugh' 'uk' 'uncle' 'understand' 'unless' 'unlimited'\n",
      " 'unredeemed' 'unsubscribe' 'update' 'ur' 'ure' 'urgent' 'urself' 'use'\n",
      " 'used' 'user' 'usf' 'using' 'uve' 'valentine' 'valid' 'valued' 'via'\n",
      " 'video' 'visit' 'voice' 'voucher' 'wait' 'waiting' 'wake' 'walk' 'wan'\n",
      " 'wana' 'wanna' 'want' 'wanted' 'wap' 'warm' 'wasnt' 'wat' 'watch'\n",
      " 'watching' 'water' 'wats' 'way' 'wed' 'weed' 'week' 'weekend' 'weekly'\n",
      " 'welcome' 'well' 'wen' 'went' 'whatever' 'whats' 'whenever' 'who' 'whole'\n",
      " 'wid' 'wif' 'wife' 'wil' 'win' 'wine' 'winner' 'wish' 'wishing' 'wit'\n",
      " 'within' 'without' 'wiv' 'wk' 'wkly' 'woke' 'woman' 'wonder' 'wonderful'\n",
      " 'wondering' 'wont' 'word' 'work' 'workin' 'working' 'world' 'worried'\n",
      " 'worry' 'worth' 'wot' 'would' 'wow' 'write' 'wrong' 'wwwgetzedcouk'\n",
      " 'xmas' 'xx' 'xxx' 'xy' 'ya' 'yar' 'yeah' 'year' 'yep' 'yes' 'yesterday'\n",
      " 'yet' 'yo' 'youll' 'youre' 'youve' 'yr' 'yup']\n"
     ]
    }
   ],
   "source": [
    "# Check the final processed dataset and the TF-IDF result\n",
    "print(df.head())\n",
    "print(\"TF-IDF Features:\")\n",
    "print(tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70552c61-701b-4f86-840c-b288d752b072",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
